{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIND USER'S POSTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: add refresh at every error , if the error persists after 3 refresh then quit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for scraping \n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import xlsxwriter\n",
    "\n",
    "# misc\n",
    "import re as re # regex \n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# accesing env file \n",
    "import os \n",
    "from dotenv import load_dotenv # to access the secret keys we've hidden in a separate file \n",
    "load_dotenv() # grab values inside env file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mercury Drugstore Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = os.getenv(\"WEBDRIVER_PATH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize web driver that would control the web browser\n",
    "ser = Service(PATH)\n",
    "op = webdriver.ChromeOptions()\n",
    "driver = webdriver.Chrome(service=ser, options=op)\n",
    "\n",
    "#website we wanted to access \n",
    "driver.get(\"https://www.mercurydrug.com/store-locator.html\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Web Scraping Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating lists\n",
    "branch_name = []\n",
    "branch_phone = []\n",
    "branch_address = []\n",
    "branch_store_hours = []\n",
    "\n",
    "def mercury_drugs_scrape():\n",
    "    group_iteration = 4\n",
    "    group_frames = {\n",
    "        \"0\" : [\"luzframe\",\"luzbranchframe\",\"luzdetailframe\"], \n",
    "        \"1\" : [\"mmframe\",\"mmbranchframe\",\"mmdetailframe\"], \n",
    "        \"2\" : [\"visframe\",\"visbranchframe\",\"visdetailframe\"], \n",
    "        \"3\" : [\"minframe\",\"minbranchframe\",\"mindetailframe\"],\n",
    "    }\n",
    "    group_frames_iteration = 3\n",
    "    \n",
    "    groups_size = range(1)\n",
    "\n",
    "    for group in groups_size:\n",
    "\n",
    "        # select group \n",
    "        driver.execute_script(\"arguments[0].click();\", WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH, \"//body/div[@class='container']/div/div/ul[@class='cf sf-loc-menu']/li[@data-tab='tab-{}']\".format(group_iteration)))))\n",
    "\n",
    "        # grab list of provinces \n",
    "        driver.switch_to.frame(group_frames[\"{}\".format(group_frames_iteration)][0])\n",
    "        iframe_province_page = driver.page_source\n",
    "        soup = bs(iframe_province_page.encode(\"utf-8\"), \"html\")\n",
    "        soup.prettify()\n",
    "        provinces = soup.findAll(\"li\",{\"class\":None})\n",
    "\n",
    "        provinceIteration = 1\n",
    "        for province in provinces:\n",
    "            # select province\n",
    "            driver.find_element(By.CSS_SELECTOR,\"li:nth-child({})\".format(provinceIteration)).click()\n",
    "\n",
    "            # grab list of cities \n",
    "            driver.switch_to.parent_frame() # go back to original frame \n",
    "            driver.switch_to.frame(group_frames[\"{}\".format(group_frames_iteration)][1])\n",
    "            iframe_cities_page = driver.page_source\n",
    "            soup = bs(iframe_cities_page.encode(\"utf-8\"), \"html\")\n",
    "            soup.prettify()\n",
    "            cities_box = soup.find(\"select\",{\"id\":\"sel1\"})\n",
    "            cities = cities_box.findAll(\"option\",{\"class\":None})\n",
    "            cities = cities[1:]\n",
    "\n",
    "            cityIteration = 2\n",
    "            for city in cities: \n",
    "                time.sleep(1)\n",
    "                # select city \n",
    "                driver.find_element(By.CSS_SELECTOR,\"option:nth-child({})\".format(cityIteration)).click()\n",
    "\n",
    "                # data scraping - here's where the fun starts \n",
    "                # switch to details frame, to grab details for each city \n",
    "                driver.switch_to.parent_frame() # go back to original frame \n",
    "                driver.switch_to.frame(group_frames[\"{}\".format(group_frames_iteration)][2])\n",
    "                iframe_details_page = driver.page_source\n",
    "                soup = bs(iframe_details_page.encode(\"utf-8\"), \"html\")\n",
    "                soup.prettify()\n",
    "                \n",
    "                # grab the values \n",
    "                details_box = soup.find(\"div\",{\"class\":\"prov-details\"})\n",
    "                details_table = details_box.findAll(\"tr\")\n",
    "                branch_name_value = details_box.find(\"h4\",{\"class\":None}).get_text()\n",
    "                branch_address_value = details_table[0].select(\"td:nth-child(2)\")[0].get_text()\n",
    "                branch_phone_value = details_table[1].select(\"td:nth-child(2)\")[0].get_text()\n",
    "                branch_store_hours_value = details_table[2].select(\"td:nth-child(2)\")[0].get_text()\n",
    "                \n",
    "                # append values \n",
    "                branch_name.append(branch_name_value)\n",
    "                branch_phone.append(branch_phone_value)\n",
    "                branch_address.append(branch_address_value)\n",
    "                branch_store_hours.append(branch_store_hours_value)\n",
    "                \n",
    "                driver.switch_to.parent_frame() # go back to original frame \n",
    "                driver.switch_to.frame(group_frames[\"{}\".format(group_frames_iteration)][1])\n",
    "\n",
    "                # stop iterating once all cities are scraped\n",
    "                option_size = len(city.findAll(\"option\",{\"class\":None})[1:])\n",
    "\n",
    "                if (cityIteration <= option_size + 1):\n",
    "                    break\n",
    "\n",
    "                cityIteration += 1\n",
    "                continue\n",
    "\n",
    "            # stop iterating once all province are scraped \n",
    "            # go back to province iframe \n",
    "            driver.switch_to.parent_frame() \n",
    "            driver.switch_to.frame(group_frames[\"{}\".format(group_frames_iteration)][0])\n",
    "            provinceIteration += 1\n",
    "            continue\n",
    "\n",
    "        # Storing scraped data into specified file format\n",
    "        data = {\n",
    "            \"Name\": branch_name,\n",
    "            \"Address\": branch_address,\n",
    "            \"Phone\": branch_phone,\n",
    "            \"Store Hours\": branch_store_hours\n",
    "        }\n",
    "\n",
    "        # save to excel \n",
    "        df = pd.DataFrame(data)\n",
    "        writer = pd.ExcelWriter(\"../data/mercury - mindanao.xlsx\", engine='xlsxwriter')\n",
    "        df.to_excel(writer, index =False)\n",
    "        writer.save()\n",
    "        \n",
    "        if (group_iteration == groups_size):\n",
    "            driver.quit()\n",
    "            break\n",
    "        \n",
    "        driver.switch_to.parent_frame() \n",
    "        group_iteration += 1\n",
    "        group_frames_iteration += 1\n",
    "        continue         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execute scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mercury_drugs_scrape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Storing scraped data into specified file format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = {\n",
    "#     \"Name\": branch_name,\n",
    "#     \"Address\": branch_address,\n",
    "#     \"Phone\": branch_phone,\n",
    "#     \"Store Hours\": branch_store_hours\n",
    "# }\n",
    "\n",
    "# # save to excel \n",
    "# df = pd.DataFrame(data)\n",
    "# writer = pd.ExcelWriter(\"../data/mercury.xlsx\", engine='xlsxwriter')\n",
    "# df.to_excel(writer, index =False)\n",
    "# writer.save()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bfb4883d108fc92ac768439090a2e92bb9a1f760a54beeecfd6762b5dcd70fe3"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
